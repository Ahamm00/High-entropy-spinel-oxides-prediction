{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78337ea8-e75b-40ad-9418-98a3ed12a254",
   "metadata": {},
   "source": [
    "# For SVM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe8cfae-fa44-4cb6-bcb3-f2ea0122db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.inspection import permutation_importance\n",
    "from skopt import BayesSearchCV\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ===================== Load Dataset =====================\n",
    "df = pd.read_csv(\"Name of dataset.csv\").drop_duplicates()\n",
    "X = df.drop(columns=['Phase', 'composition'], errors='ignore')\n",
    "y = df['Phase']\n",
    "\n",
    "# ===================== Train-Test Split =====================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ===================== Bayesian Optimization =====================\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC(class_weight='balanced', random_state=42))])\n",
    "param_space = {\n",
    "    'svc__C': (1e-3, 1e3, 'log-uniform'),\n",
    "    'svc__gamma': (1e-4, 1e1, 'log-uniform'),\n",
    "    'svc__kernel': ['rbf']\n",
    "}\n",
    "cv_bayes = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "bayes = BayesSearchCV(pipe, param_space, n_iter=30, cv=cv_bayes, scoring='f1_weighted', n_jobs=-1, random_state=42)\n",
    "print(\"üîÑ Running Bayesian Optimization...\")\n",
    "bayes.fit(X_train, y_train)\n",
    "\n",
    "best_params = bayes.best_params_\n",
    "print(\"‚úÖ Best Hyperparameters:\", best_params)\n",
    "\n",
    "# ===================== Fix SVM with Best Hyperparameters =====================\n",
    "best_pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC(\n",
    "    C=best_params['svc__C'],\n",
    "    gamma=best_params['svc__gamma'],\n",
    "    kernel='rbf',\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    "))])\n",
    "\n",
    "# ===================== CV-Aware Permutation Importance (10 folds √ó 100 repeats) =====================\n",
    "cv_perm = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "importances_cv = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv_perm.split(X_train, y_train), start=1):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    best_pipe.fit(X_tr, y_tr)\n",
    "    perm = permutation_importance(best_pipe, X_val, y_val, n_repeats=100, scoring='f1_weighted', random_state=42, n_jobs=-1)\n",
    "    importances_cv.append(perm.importances_mean)\n",
    "\n",
    "importances_cv = np.array(importances_cv)\n",
    "mean_importance = importances_cv.mean(axis=0)\n",
    "top_features = X_train.columns[np.argsort(mean_importance)[::-1]][:40]\n",
    "\n",
    "X_train = X_train[top_features]\n",
    "X_test = X_test[top_features]\n",
    "\n",
    "# ===================== Correlation Pruning =====================\n",
    "corr_matrix = X_train.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [col for col in upper_tri.columns if any(upper_tri[col] > 0.95)]\n",
    "X_train = X_train.drop(columns=to_drop)\n",
    "X_test = X_test.drop(columns=to_drop)\n",
    "\n",
    "# ===================== True Greedy Forward Selection with CV =====================\n",
    "selected_features, remaining_features = [], list(X_train.columns)\n",
    "cv_f1_history, test_f1_history = [], []\n",
    "best_cv_f1, no_improve_count, patience, min_delta = -np.inf, 0, 15, 0.001\n",
    "cv_fs = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "while remaining_features:\n",
    "    best_feature, best_step_f1 = None, -np.inf\n",
    "    for feat in remaining_features:\n",
    "        trial_features = selected_features + [feat]\n",
    "        mean_cv_f1 = cross_val_score(best_pipe, X_train[trial_features], y_train, cv=cv_fs, scoring='f1_weighted', n_jobs=-1).mean()\n",
    "        if mean_cv_f1 > best_step_f1:\n",
    "            best_step_f1, best_feature = mean_cv_f1, feat\n",
    "\n",
    "    selected_features.append(best_feature)\n",
    "    remaining_features.remove(best_feature)\n",
    "    cv_f1_history.append(best_step_f1)\n",
    "\n",
    "    best_pipe.fit(X_train[selected_features], y_train)\n",
    "    test_f1_history.append(f1_score(y_test, best_pipe.predict(X_test[selected_features]), average='weighted'))\n",
    "\n",
    "    if best_step_f1 > best_cv_f1 + min_delta:\n",
    "        best_cv_f1, no_improve_count = best_step_f1, 0\n",
    "    else:\n",
    "        no_improve_count += 1\n",
    "    if no_improve_count >= patience:\n",
    "        break\n",
    "\n",
    "best_idx = np.argmax(cv_f1_history)\n",
    "best_features = selected_features[:best_idx+1]\n",
    "\n",
    "# ===================== Final Test Evaluation =====================\n",
    "best_pipe.fit(X_train[best_features], y_train)\n",
    "y_test_pred = best_pipe.predict(X_test[best_features])\n",
    "\n",
    "best_cv_score = cv_f1_history[best_idx]\n",
    "\n",
    "print(\"\\n================ FINAL TEST PERFORMANCE =================\")\n",
    "print(f\"Number of features : {len(best_features)}\")\n",
    "print(f\"Selected features  : {best_features}\")\n",
    "print(f\"Mean CV F1-weighted: {best_cv_score:.4f}\")\n",
    "print(f\"Test F1-weighted   : {f1_score(y_test, y_test_pred, average='weighted'):.4f}\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Non-single phase','Single phase']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd01f896-9b7d-4235-8b29-09c9c0761867",
   "metadata": {},
   "source": [
    "# For prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e29d0b-6d6c-4b19-9c1e-58122eb86de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# ===================== Load Data =====================\n",
    "df_train = pd.read_csv(\"Name of training dataset.csv\").drop_duplicates()\n",
    "df_pred  = pd.read_csv(\"Name of prediction dataset.csv\")\n",
    "\n",
    "# ===================== Features =====================\n",
    "features = ['AR (A+B)', 'IE (A+B)', 'Delta (%)', 'AW(A+X)', 'AR(X/A)', \n",
    "            'AR(B/A)', 'AW(A+B)', 'AD(A-B)', 'AR(A-B)', 'AD(A-X)', \n",
    "            'VEC', 'IE (A-B)', 'AD(A+B)'] #Optimised feature set\n",
    "\n",
    "X_train, y_train = df_train[features], df_train['Phase']\n",
    "X_pred = df_pred[features]\n",
    "\n",
    "# ===================== Train SVM =====================\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC(C=1000, gamma=0.003726997693542054, kernel='rbf',\n",
    "                class_weight='balanced', probability=True, random_state=42))\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# ===================== Predict Probabilities =====================\n",
    "proba = pipeline.predict_proba(X_pred)[:, 1]\n",
    "df_pred['Single_phase_proba'] = proba\n",
    "\n",
    "# ===================== Apply Thresholds =====================\n",
    "for t in [0.5, 0.6, 0.7]:\n",
    "    df_pred[f'Pred_Phase_p{int(t*100)}'] = (proba >= t).astype(int)\n",
    "\n",
    "# ===================== Save Results =====================\n",
    "df_pred.to_csv(\"HESO_pred_cv10_Stratified.csv\", index=False)\n",
    "print(\"‚úÖ Predictions saved with thresholds 0.5, 0.6, 0.7\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78d641d-4e3f-434b-8362-51a338d46965",
   "metadata": {},
   "source": [
    "# CHGNet relaxation code after SQS generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b2040f-1537-4683-a604-b307cff27331",
   "metadata": {},
   "source": [
    "### The code assumes that the SQS generated structures are named as HEO1.vasp, HEO2.vasp...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c975a6cd-b5f1-449f-a3a2-e2fcb9540194",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas pymatgen chgnet numpy tqdm\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from pymatgen.core import Structure\n",
    "from chgnet.model import StructOptimizer, CHGNet\n",
    "\n",
    "# ------------------------\n",
    "# CONFIGURATION\n",
    "# ------------------------\n",
    "start_idx = 1     # Start HEO index (inclusive)\n",
    "end_idx = 5       # End HEO index (inclusive)\n",
    "\n",
    "base_path = \"/content/drive/MyDrive/HEO_predicted\"\n",
    "relax_dir = \"relaxed_structures\"\n",
    "os.makedirs(relax_dir, exist_ok=True)\n",
    "\n",
    "# ------------------------\n",
    "# Load CHGNet Model\n",
    "# ------------------------\n",
    "print(\"üîß Initializing CHGNet model...\")\n",
    "chgnet_model = CHGNet.load()\n",
    "chgnet_relaxer = StructOptimizer()\n",
    "\n",
    "# ------------------------\n",
    "# Relax Structures\n",
    "# ------------------------\n",
    "csv_path = \"relaxation_results.csv\"\n",
    "all_entries = []\n",
    "\n",
    "print(f\"\\nüîÑ Starting CHGNet relaxation for HEO{start_idx} to HEO{end_idx}...\\n\")\n",
    "\n",
    "for i in range(start_idx, end_idx + 1):\n",
    "    heo_file = f\"HEO{i}.vasp\"\n",
    "    vasp_path = os.path.join(base_path, heo_file)\n",
    "\n",
    "    if not os.path.exists(vasp_path):\n",
    "        print(f\"‚ö†Ô∏è  Skipping missing file: {heo_file}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        struct = Structure.from_file(vasp_path)\n",
    "        print(f\"üîß Relaxing {heo_file}...\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        result = chgnet_relaxer.relax(struct, fmax=0.005, relax_cell=True, verbose=True)\n",
    "        end_time = time.time()\n",
    "\n",
    "        relaxed_struct = result[\"final_structure\"]\n",
    "        final_energy = float(result[\"trajectory\"].energies[-1])\n",
    "        final_fmax = float(result[\"trajectory\"].forces[-1].max())\n",
    "        relax_time_sec = end_time - start_time\n",
    "\n",
    "        # Extract lattice information\n",
    "        lattice = relaxed_struct.lattice\n",
    "        a, b, c = lattice.a, lattice.b, lattice.c\n",
    "        alpha, beta, gamma = lattice.alpha, lattice.beta, lattice.gamma\n",
    "        volume = lattice.volume\n",
    "\n",
    "        save_name = heo_file.replace(\".vasp\", \"_rlx.vasp\")\n",
    "        save_path = os.path.join(relax_dir, save_name)\n",
    "        relaxed_struct.to(fmt=\"poscar\", filename=save_path)\n",
    "\n",
    "        row = {\n",
    "            \"HEO_file\": heo_file,\n",
    "            \"relaxed_file\": save_name,\n",
    "            \"relaxed_energy (eV)\": final_energy,\n",
    "            \"a (√Ö)\": a,\n",
    "            \"b (√Ö)\": b,\n",
    "            \"c (√Ö)\": c,\n",
    "            \"alpha (¬∞)\": alpha,\n",
    "            \"beta (¬∞)\": beta,\n",
    "            \"gamma (¬∞)\": gamma,\n",
    "            \"volume (√Ö¬≥)\": volume,\n",
    "            \"time_taken (s)\": relax_time_sec,\n",
    "            \"final_fmax (eV/√Ö)\": final_fmax\n",
    "        }\n",
    "        all_entries.append(row)\n",
    "        print(f\"  ‚úÖ Relaxed {heo_file} | Energy = {final_energy:.4f} eV | Time = {relax_time_sec:.1f} s\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Relaxation failed for {heo_file}: {e}\")\n",
    "\n",
    "# ------------------------\n",
    "# Append to CSV\n",
    "# ------------------------\n",
    "if all_entries:\n",
    "    df = pd.DataFrame(all_entries)\n",
    "    if os.path.exists(csv_path):\n",
    "        df.to_csv(csv_path, mode='a', index=False, header=False)\n",
    "    else:\n",
    "        df.to_csv(csv_path, index=False)\n",
    "    print(f\"üíæ Appended results to: {csv_path}\")\n",
    "else:\n",
    "    print(\"‚ùó No successful relaxations to record.\")\n",
    "\n",
    "print(\"\\nüéâ Task complete!\")\n",
    "\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from google.colab import files\n",
    "\n",
    "# === ZIP the relaxed structures folder ===\n",
    "zip_filename = \"relaxed_structures.zip\"\n",
    "with ZipFile(zip_filename, 'w') as zipf:\n",
    "    for root, _, files_list in os.walk(relax_dir):\n",
    "        for file in files_list:\n",
    "            filepath = os.path.join(root, file)\n",
    "            arcname = os.path.relpath(filepath, relax_dir)\n",
    "            zipf.write(filepath, arcname=os.path.join(\"relaxed_structures\", arcname))\n",
    "\n",
    "# === Download the zip and the CSV ===\n",
    "print(\"\\nüì¶ Downloading results...\")\n",
    "files.download(zip_filename)         # relaxed structures\n",
    "files.download(csv_path)             # relaxation_results.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f28df1-e507-49af-a83b-349c7cdf72a6",
   "metadata": {},
   "source": [
    "# SynthNN score prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1885fa4-c74b-4f2c-9eba-f6141caee88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/antoniuk1/SynthNN.git\n",
    "%cd SynthNN\n",
    "\n",
    "#from SynthNN.utils import synthNN_predict\n",
    "import numpy as np\n",
    "from SynthNN.utils import get_features, synthNN_predict\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "#Option 2: Read in formulas from text file\n",
    "data=np.loadtxt('input_formulas.txt',dtype=str) #Paste the composition like Mg0.2Mn0.2Cu0.2Ni0.2Zn0.2Al2O4\n",
    "output_file_name='output_formula_preds.txt' \n",
    "saved_model_dir='./Trained_models/Paper_final_model/'\n",
    "\n",
    "synthNN_predict(data,output_file_name, saved_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075e0b15-ea18-4e89-8436-6837cf9e87cb",
   "metadata": {},
   "source": [
    "# Code for XRD generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdda3a4-2a00-4f71-b7a0-d777dbca74ff",
   "metadata": {},
   "source": [
    "### This code assumes you have a folder named 'relaxed_structures_fil with CHGNet relaxed structures and material ID (HEOx_rlx.vasp) x=1,2,3... Rest other csv's are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8d202c-d9c0-42bf-b79b-21ec3f9b2e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pymatgen.core import Structure\n",
    "from pymatgen.analysis.diffraction.xrd import XRDCalculator\n",
    "from pymatgen.ext.matproj import MPRester\n",
    "import os\n",
    "\n",
    "# === Your MP API Key (paste it here) ===\n",
    "MAPI_KEY = \"Your API Key\"\n",
    "\n",
    "# === Initialize objects ===\n",
    "mpr = MPRester(MAPI_KEY)\n",
    "xrd_calc = XRDCalculator(wavelength=\"CuKa\")  # default Œª = 1.5406 √Ö\n",
    "\n",
    "# === Peak filtering function ===\n",
    "def filter_peaks(pattern, threshold=0.04):\n",
    "    max_intensity = max(pattern.y)\n",
    "    return [(round(t, 2), round(i / max_intensity, 3)) \n",
    "            for t, i in zip(pattern.x, pattern.y) if i / max_intensity >= threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2552813d-5cd1-44ac-b9c1-451815e06a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(ref_peaks, test_peaks, theta_tol=0.6, intensity_diff_weight=0.1):\n",
    "    \"\"\"\n",
    "    Compare two XRD patterns using only peak positions (2Œ∏) and intensities.\n",
    "    Penalizes:\n",
    "    - missing reference peaks (0.05 per peak)\n",
    "    - extra peaks in test with intensity > 0.1\n",
    "    - intensity differences between matched peaks (scaled by intensity_diff_weight)\n",
    "\n",
    "    Parameters:\n",
    "        ref_peaks (list of (theta, intensity)) - Reference pattern (e.g., from Materials Project)\n",
    "        test_peaks (list of (theta, intensity)) - Test pattern (e.g., from CHGNet-relaxed structure)\n",
    "        theta_tol (float) - Matching tolerance for 2Œ∏ in degrees\n",
    "        intensity_diff_weight (float) - weight factor for intensity difference penalty\n",
    "\n",
    "    Returns:\n",
    "        similarity_score (float between 0 and 1)\n",
    "    \"\"\"\n",
    "    penalty = 0.0\n",
    "\n",
    "    # === Match each reference peak and accumulate intensity difference penalty ===\n",
    "    matched_test_indices = set()\n",
    "    for i_ref, (theta_ref, inten_ref) in enumerate(ref_peaks):\n",
    "        matched = False\n",
    "        for i_test, (theta_test, inten_test) in enumerate(test_peaks):\n",
    "            if abs(theta_test - theta_ref) <= theta_tol:\n",
    "                matched = True\n",
    "                matched_test_indices.add(i_test)\n",
    "                # Intensity difference penalty (normalized intensities between 0-1)\n",
    "                inten_diff = abs(inten_ref - inten_test)\n",
    "                penalty += inten_diff * intensity_diff_weight\n",
    "                break\n",
    "        if not matched:\n",
    "            penalty += 0.05  # Penalize for each unmatched reference peak\n",
    "\n",
    "    # === Penalize extra peaks in test with intensity > 0.1 ===\n",
    "    for i_test, (theta_test, inten_test) in enumerate(test_peaks):\n",
    "        if i_test not in matched_test_indices and inten_test > 0.1:\n",
    "            penalty += 0.005\n",
    "\n",
    "    # === Final similarity score ===\n",
    "    score = 1 / (1 + penalty)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7a22cc-6a3b-418a-b973-fa64d3c6c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load input data ===\n",
    "relaxed_df = pd.read_csv(\"Final selected structures.csv\") #CSV with final selected structures composition (A1-A5 & B-site), SVM score, SynthNN score\n",
    "mp_df = pd.read_csv(\"normal_spinels.csv\") \n",
    "\n",
    "# Filter out rows where Synthpred < 0.6\n",
    "#relaxed_df = relaxed_df[relaxed_df['SynthPred'] >= 0.6]\n",
    "\n",
    "# === For trial, limit to first 5 ===\n",
    "start_idx = 0\n",
    "end_idx = 38\n",
    "trial_df = relaxed_df.iloc[start_idx:end_idx]\n",
    "\n",
    "results = []\n",
    "patterns_to_plot = []\n",
    "\n",
    "for _, row in trial_df.iterrows():\n",
    "    relaxed_path = os.path.join(\"relaxed_structures_fil\", row[\"relaxed_file\"])\n",
    "    B_site = row[\"B\"]\n",
    "    \n",
    "    # Load relaxed structure\n",
    "    try:\n",
    "        struct_rlx = Structure.from_file(relaxed_path)\n",
    "        xrd_rlx = xrd_calc.get_pattern(struct_rlx)\n",
    "        peaks_rlx = filter_peaks(xrd_rlx)\n",
    "    except:\n",
    "        print(f\"Error loading: {relaxed_path}\")\n",
    "        continue\n",
    "\n",
    "    # Find MP reference spinels with same B\n",
    "    matches = mp_df[mp_df[\"B\"] == B_site]\n",
    "    best_score = -1\n",
    "    best_mp_id = None\n",
    "    best_formula = None\n",
    "    best_ref_peaks = []\n",
    "\n",
    "    for _, mp_row in matches.iterrows():\n",
    "        mp_id = mp_row[\"material_id\"]\n",
    "        try:\n",
    "            struct_mp = mpr.get_structure_by_material_id(mp_id)\n",
    "            xrd_mp = xrd_calc.get_pattern(struct_mp)\n",
    "            peaks_mp = filter_peaks(xrd_mp)\n",
    "            score = compute_similarity(peaks_mp, peaks_rlx)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_mp_id = mp_id\n",
    "                best_formula = mp_row[\"formula\"]\n",
    "                best_ref_peaks = peaks_mp\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    results.append({\n",
    "        \"relaxed_file\": row[\"relaxed_file\"],\n",
    "        \"best_mp_id\": best_mp_id,\n",
    "        \"best_formula\": best_formula,\n",
    "        \"similarity_score\": best_score\n",
    "    })\n",
    "    \n",
    "    patterns_to_plot.append((row[\"relaxed_file\"], peaks_rlx, best_mp_id, best_ref_peaks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4314a568-6920-4141-9802-68d50a437101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "def sanitize_filename(s):\n",
    "    # Replace any character not alphanumeric or _ with _\n",
    "    return re.sub(r'[^a-zA-Z0-9_-]', '_', s.replace(' ', '_'))\n",
    "\n",
    "def to_subscript(s):\n",
    "    sub_map = str.maketrans(\"0123456789.\", \"‚ÇÄ‚ÇÅ‚ÇÇ‚ÇÉ‚ÇÑ‚ÇÖ‚ÇÜ‚Çá‚Çà‚Çâ‚ãÖ\")\n",
    "    return s.translate(sub_map)\n",
    "\n",
    "plots_per_fig = 25\n",
    "n_plots = len(patterns_to_plot)\n",
    "n_figs = (n_plots - 1) // plots_per_fig + 1\n",
    "\n",
    "for fig_idx in range(n_figs):\n",
    "    start_idx = fig_idx * plots_per_fig\n",
    "    end_idx = min(start_idx + plots_per_fig, n_plots)\n",
    "    current_patterns = patterns_to_plot[start_idx:end_idx]\n",
    "    \n",
    "    fig, axes = plt.subplots(5, 5, figsize=(25, 25), dpi=500)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (relaxed_file, peaks_rlx, mp_id, peaks_mp) in enumerate(current_patterns):\n",
    "        ax = axes[i]\n",
    "\n",
    "        # XRD peak positions and normalized intensities\n",
    "        x_rlx, y_rlx = zip(*peaks_rlx) if peaks_rlx else ([], [])\n",
    "        x_mp, y_mp = zip(*peaks_mp) if peaks_mp else ([], [])\n",
    "\n",
    "        # Plot relaxed (HEO) structure ‚Äî solid blue sticks\n",
    "        for x, y in zip(x_rlx, y_rlx):\n",
    "            ax.vlines(x, 0, y, color='blue', linewidth=1.5)\n",
    "\n",
    "        # Plot MP reference structure ‚Äî dashed red sticks\n",
    "        for x, y in zip(x_mp, y_mp):\n",
    "            ax.vlines(x, 0, y, color='red', linestyle='dashed', linewidth=1.5)\n",
    "\n",
    "        # Add baseline at y=0\n",
    "        ax.axhline(0, color='black', linewidth=0.8)\n",
    "\n",
    "        # Get composition string from relaxed_df\n",
    "        base_name = relaxed_file.replace('_rlx.vasp', '')\n",
    "        composition = relaxed_df.loc[relaxed_df['relaxed_file'] == relaxed_file, 'composition'].values[0]\n",
    "        composition_sub = to_subscript(composition)\n",
    "\n",
    "        # Get MP formula and subscript\n",
    "        mp_row = next((r for r in results if r[\"relaxed_file\"] == relaxed_file), None)\n",
    "        mp_formula = mp_row[\"best_formula\"] if mp_row else \"?\"\n",
    "        mp_formula_sub = to_subscript(mp_formula)\n",
    "        mp_id_display = mp_row[\"best_mp_id\"] if mp_row else \"?\"\n",
    "\n",
    "        # Similarity score for title\n",
    "        score = mp_row[\"similarity_score\"]\n",
    "        score_str = f\"{score:.2f}\" if score is not None else \"N/A\"\n",
    "\n",
    "        # Title and labels\n",
    "        title_str = f\"{base_name} vs {mp_formula} ({score_str})\"\n",
    "        ax.set_title(title_str, fontsize=10)\n",
    "        ax.set_xlabel(\"2Œ∏ (¬∞)\", fontsize=9)\n",
    "        ax.set_ylabel(\"Normalized Intensity\", fontsize=9)\n",
    "        ax.set_xlim(20, 60)\n",
    "\n",
    "        # Legend with subscripts\n",
    "        legend_labels = [\n",
    "            f\"{composition_sub}\",\n",
    "            f\"{mp_formula_sub} ({mp_id_display})\"\n",
    "        ]\n",
    "        ax.legend(legend_labels, fontsize=8)\n",
    "\n",
    "        # Save individual subplot as separate figure\n",
    "        fig_single, ax_single = plt.subplots(figsize=(8, 5), dpi=500)\n",
    "\n",
    "        # Plot same data in individual subplot figure\n",
    "        for x, y in zip(x_rlx, y_rlx):\n",
    "            ax_single.vlines(x, 0, y, color='blue', linewidth=1.5)\n",
    "        for x, y in zip(x_mp, y_mp):\n",
    "            ax_single.vlines(x, 0, y, color='red', linestyle='dashed', linewidth=1.5)\n",
    "        ax_single.axhline(0, color='black', linewidth=0.8)\n",
    "\n",
    "        ax_single.set_title(title_str, fontsize=12)\n",
    "        ax_single.set_xlabel(\"2Œ∏ (¬∞)\", fontsize=10)\n",
    "        ax_single.set_ylabel(\"Normalized Intensity\", fontsize=10)\n",
    "        ax_single.set_xlim(20, 60)\n",
    "        ax_single.legend(legend_labels, fontsize=10)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Sanitize filename and save\n",
    "        filename = sanitize_filename(title_str) + \".tiff\"\n",
    "        fig_single.savefig(filename, dpi=500)\n",
    "        plt.close(fig_single)\n",
    "\n",
    "    # Hide any unused subplots in the grid\n",
    "    for j in range(i + 1, plots_per_fig):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    combined_filename = f\"XRD_similarity_combined_part_{fig_idx+1}.tiff\"\n",
    "    fig.savefig(combined_filename, dpi=500)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
